#!/usr/bin/env python3
"""
note.comè¨˜äº‹å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

note.comã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰å…¨è¨˜äº‹ã‚’å–å¾—ã—ã€Markdownå½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚
ç”»åƒã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚
"""

import argparse
import json
import logging
import re
import time
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
import html2text
from slugify import slugify
import yaml
from dateutil import parser as date_parser


# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
@dataclass
class Article:
    """è¨˜äº‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    id: str
    key: str
    title: str
    publish_at: datetime
    eyecatch_url: Optional[str] = None
    url: str = ""


@dataclass
class ArticleDetail(Article):
    """è©³ç´°ãªè¨˜äº‹ãƒ‡ãƒ¼ã‚¿"""
    body_html: str = ""
    body_markdown: str = ""
    image_urls: List[str] = field(default_factory=list)
    like_count: int = 0
    json_ld: Dict = field(default_factory=dict)
    date_modified: Optional[str] = None  # ISO 8601å½¢å¼ã®æ›´æ–°æ—¥æ™‚


class ArticleParser:
    """HTMLè§£æã¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""

    @staticmethod
    def extract_json_ld(html: str) -> dict:
        """JSON-LDã‚¹ã‚­ãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html, 'lxml')
        scripts = soup.find_all('script', {'type': 'application/ld+json'})

        for script in scripts:
            if not script.string:
                continue
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    # @graphãŒã‚ã‚‹å ´åˆï¼ˆSchema.orgæ§‹é€ ï¼‰
                    if '@graph' in data:
                        for item in data['@graph']:
                            if isinstance(item, dict) and item.get('@type') == 'BlogPosting':
                                return item
                    # ç›´æ¥BlogPostingã®å ´åˆ
                    elif data.get('@type') == 'BlogPosting':
                        return data
                elif isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict) and item.get('@type') == 'BlogPosting':
                            return item
            except json.JSONDecodeError as e:
                logger.debug(f"JSON-LD parse error: {e}")
                continue

        return {}

    @staticmethod
    def extract_article_list_from_profile(html: str) -> List[dict]:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹ãƒªã‚¹ãƒˆã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html, 'lxml')
        articles = []

        # initialLatestNoteDataã‹ã‚‰noteKeysã‚’æŠ½å‡ºï¼ˆå„ªå…ˆï¼šç¢ºå®Ÿã«å–å¾—ã§ãã‚‹ï¼‰
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: initialLatestNoteData\":{\"noteKeys\":[\"na3f6f4e2138e\",...]
        match = re.search(r'initialLatestNoteData\\":\{\\"noteKeys\\":\[([^\]]+)\]', html)
        if match:
            note_keys_str = match.group(1)
            # [\"na3f6f4e2138e\",\"n344e56a81c58\",...] ã‹ã‚‰æŠ½å‡º
            note_keys = re.findall(r'\\"([a-z0-9]+)\\"', note_keys_str)

            logger.debug(f"Found {len(note_keys)} note keys: {note_keys}")

            for note_key in note_keys:
                articles.append({
                    'id': note_key,
                    'key': note_key,
                    'name': None,  # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
                    'publishAt': None,  # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
                    'eyecatch': None
                })

            if articles:
                return articles

        # Next.jsã® __NEXT_DATA__ ã‹ã‚‰æŠ½å‡ºã‚’è©¦ã¿ã‚‹
        scripts = soup.find_all('script', {'id': '__NEXT_DATA__'})
        for script in scripts:
            if not script.string:
                continue
            try:
                data = json.loads(script.string)
                props = data.get('props', {}).get('pageProps', {})

                # è¨˜äº‹ãƒªã‚¹ãƒˆã‚’æ¢ã™
                user_contents = props.get('userContents', {})
                contents = user_contents.get('contents', [])

                for content in contents:
                    if content.get('type') == 'Note':
                        articles.append({
                            'id': content.get('id'),
                            'key': content.get('key'),
                            'name': content.get('name'),
                            'publishAt': content.get('publishAt'),
                            'eyecatch': content.get('eyecatch')
                        })

                if articles:
                    return articles

            except json.JSONDecodeError as e:
                logger.debug(f"__NEXT_DATA__ parse error: {e}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: HTMLã‹ã‚‰è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        article_links = soup.select('a[href*="/n/"]')
        seen_keys = set()

        for link in article_links:
            href = link.get('href', '')
            match = re.search(r'/n/([a-z0-9]+)', href)
            if match:
                key = match.group(1)
                if key not in seen_keys:
                    seen_keys.add(key)
                    title_elem = link.select_one('.note-title, h2, h3')
                    title = title_elem.get_text(strip=True) if title_elem else "Untitled"

                    articles.append({
                        'id': key,
                        'key': key,
                        'name': title,
                        'publishAt': None,
                        'eyecatch': None
                    })

        return articles

    @staticmethod
    def extract_article_body(soup: BeautifulSoup) -> str:
        """è¨˜äº‹æœ¬æ–‡HTMLã‚’æŠ½å‡º"""
        # å„ªå…ˆã‚»ãƒ¬ã‚¯ã‚¿
        selectors = [
            'div.p-article__body',
            'article.note-common-styles__textnote-body',
            'div.note-common-styles__textnote-body',
            'article',
            '[role="article"]'
        ]

        for selector in selectors:
            body = soup.select_one(selector)
            if body:
                return str(body)

        raise ValueError("è¨˜äº‹æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


class HTMLToMarkdownConverter:
    """HTMLâ†’Markdownå¤‰æ›"""

    def __init__(self):
        self.h2md = html2text.HTML2Text()
        self.h2md.body_width = 0  # è‡ªå‹•æ”¹è¡Œç„¡åŠ¹
        self.h2md.ignore_links = False
        self.h2md.ignore_images = False
        self.h2md.ignore_emphasis = False

    def convert(self, html: str) -> str:
        """HTMLã‚’Markdownã«å¤‰æ›"""
        markdown = self.h2md.handle(html)
        # ä½™åˆ†ãªç©ºè¡Œã‚’å‰Šé™¤
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)
        return markdown.strip()

    def extract_image_urls(self, html: str) -> List[str]:
        """HTMLå†…ã®ç”»åƒURLã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html, 'lxml')
        images = []

        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src:
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                if not src.startswith('http'):
                    src = urljoin('https://note.com', src)
                images.append(src)

        # èƒŒæ™¯ç”»åƒã‚‚ãƒã‚§ãƒƒã‚¯
        for elem in soup.find_all(style=re.compile(r'background-image')):
            style = elem.get('style', '')
            urls = re.findall(r'url\(["\']?([^"\'()]+)["\']?\)', style)
            for url in urls:
                if not url.startswith('http'):
                    url = urljoin('https://note.com', url)
                images.append(url)

        return list(set(images))  # é‡è¤‡é™¤å»


class ImageDownloader:
    """ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ç®¡ç†"""

    def __init__(self, base_image_dir: Path):
        self.base_image_dir = base_image_dir
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def download_images(self, article_id: str, image_urls: List[str]) -> Dict[str, str]:
        """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦URLâ†’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¿”ã™"""
        article_dir = self.base_image_dir / article_id
        article_dir.mkdir(parents=True, exist_ok=True)

        url_map = {}

        for idx, url in enumerate(image_urls, start=1):
            try:
                # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                response = self.session.get(url, timeout=30)
                response.raise_for_status()

                # æ‹¡å¼µå­ã‚’å–å¾—
                parsed_url = urlparse(url)
                ext = Path(parsed_url.path).suffix
                if not ext or ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']:
                    # Content-Typeã‹ã‚‰æ¨æ¸¬
                    content_type = response.headers.get('Content-Type', '')
                    ext_map = {
                        'image/jpeg': '.jpg',
                        'image/png': '.png',
                        'image/gif': '.gif',
                        'image/webp': '.webp',
                        'image/svg+xml': '.svg'
                    }
                    ext = ext_map.get(content_type, '.jpg')

                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
                filename = f"image_{idx}{ext}"
                filepath = article_dir / filename

                # ä¿å­˜
                filepath.write_bytes(response.content)

                # ç›¸å¯¾ãƒ‘ã‚¹ã‚’ç”Ÿæˆï¼ˆarticlesãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰è¦‹ãŸç›¸å¯¾ãƒ‘ã‚¹ï¼‰
                relative_path = f"../images/{article_id}/{filename}"
                url_map[url] = relative_path

                logger.info(f"  âœ“ ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {filename}")
                time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

            except Exception as e:
                logger.warning(f"  âœ— ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•— ({url}): {e}")
                continue

        return url_map

    def replace_image_urls(self, markdown: str, url_map: Dict[str, str]) -> str:
        """Markdownå†…ã®ç”»åƒURLã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã«ç½®æ›"""
        result = markdown
        for remote_url, local_path in url_map.items():
            # Markdownç”»åƒè¨˜æ³•
            result = result.replace(f"]({remote_url})", f"]({local_path})")
            result = result.replace(f'="{remote_url}"', f'="{local_path}"')
            # HTML img ã‚¿ã‚°
            result = result.replace(f'src="{remote_url}"', f'src="{local_path}"')
            result = result.replace(f"src='{remote_url}'", f"src='{local_path}'")

        return result


class MarkdownGenerator:
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""

    @staticmethod
    def create_frontmatter(article: ArticleDetail, day_number: int, fetched_at: datetime,
                          date_modified: Optional[str] = None) -> str:
        """YAMLãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã‚’ç”Ÿæˆ"""
        frontmatter = {
            'type': 'article',
            'source': 'note.com',
            'article_id': article.id,
            'day_number': day_number,
            'title': article.title,
            'author': 'æ¯›åˆ©è£•ä»‹',
            'publish_date': article.publish_at.strftime('%Y-%m-%d'),
            'publish_datetime': article.publish_at.isoformat(),
            'original_url': article.url,
            'status': 'published',
            'category': 'advent-calendar-2025',
            'tags': ['ã‚¢ãƒ‰ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼'],
            'created': datetime.now().strftime('%Y-%m-%d'),
            'fetched_at': fetched_at.isoformat()
        }

        # æ›´æ–°æ—¥æ™‚ã‚’è¿½åŠ ï¼ˆdateModifiedãŒã‚ã‚Œã°ï¼‰
        if date_modified:
            frontmatter['date_modified'] = date_modified

        return yaml.dump(frontmatter, allow_unicode=True, sort_keys=False)

    @staticmethod
    def parse_frontmatter(filepath: Path) -> Optional[dict]:
        """æ—¢å­˜Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã‚’è§£æ"""
        try:
            content = filepath.read_text(encoding='utf-8')
            # ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã‚’æŠ½å‡º
            match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
            if match:
                frontmatter_str = match.group(1)
                return yaml.safe_load(frontmatter_str)
        except Exception as e:
            logger.debug(f"Frontmatter parse error ({filepath.name}): {e}")
        return None

    @staticmethod
    def generate_filename(day_number: int, title: str, article_id: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ"""
        return f"day{day_number:04d}_{article_id}.md"

    @staticmethod
    def save_article(article: ArticleDetail, day_number: int, markdown_content: str,
                    output_dir: Path, fetched_at: datetime, date_modified: Optional[str] = None):
        """è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        output_dir.mkdir(parents=True, exist_ok=True)

        filename = MarkdownGenerator.generate_filename(day_number, article.title, article.id)
        filepath = output_dir / filename

        # ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼
        frontmatter = MarkdownGenerator.create_frontmatter(
            article, day_number, fetched_at, date_modified
        )

        # ãƒ•ãƒƒã‚¿ãƒ¼
        footer = f"\n\n---\n\n**åŸæ–‡URL**: [{article.url}]({article.url})\n"
        footer += f"**å…¬é–‹æ—¥**: {article.publish_at.strftime('%Yå¹´%-mæœˆ%-dæ—¥')}\n"
        if date_modified:
            # date_modifiedã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦è¡¨ç¤º
            try:
                mod_dt = date_parser.parse(date_modified)
                footer += f"**æ›´æ–°æ—¥**: {mod_dt.strftime('%Yå¹´%-mæœˆ%-dæ—¥')}\n"
            except:
                pass
        footer += f"**å–å¾—æ—¥**: {fetched_at.strftime('%Yå¹´%-mæœˆ%-dæ—¥')}\n"

        # å®Œå…¨ãªMarkdown
        full_content = f"---\n{frontmatter}---\n\n# {article.title}\n\n{markdown_content}{footer}"

        filepath.write_text(full_content, encoding='utf-8')
        logger.info(f"âœ“ ä¿å­˜å®Œäº†: {filename}")


class NoteArticleScraper:
    """ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""

    def __init__(self, username: str, base_dir: Path, image_dir: Path, output_dir: Path):
        self.username = username
        self.base_dir = base_dir
        self.image_dir = image_dir
        self.output_dir = output_dir
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

        self.parser = ArticleParser()
        self.converter = HTMLToMarkdownConverter()
        self.image_downloader = ImageDownloader(image_dir)

    def fetch_with_retry(self, url: str, max_retries: int = 3) -> requests.Response:
        """ãƒªãƒˆãƒ©ã‚¤ä»˜ãHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return response
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 404:
                    raise ValueError(f"ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {url}")
                elif e.response.status_code == 429:
                    sleep_time = 2 ** attempt
                    logger.warning(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™ä¸­ã€‚{sleep_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")
                    time.sleep(sleep_time)
                else:
                    raise
            except requests.exceptions.RequestException as e:
                if attempt == max_retries - 1:
                    raise
                logger.warning(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (è©¦è¡Œ{attempt + 1}å›ç›®): {e}")
                time.sleep(1)

        raise ValueError(f"æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’è¶…ãˆã¾ã—ãŸ: {url}")

    def fetch_article_list(self) -> List[Article]:
        """è¨˜äº‹ä¸€è¦§ã‚’å–å¾—"""
        profile_url = f"https://note.com/{self.username}"
        logger.info(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸å–å¾—ä¸­: {profile_url}")

        response = self.fetch_with_retry(profile_url)
        html = response.text

        articles_data = self.parser.extract_article_list_from_profile(html)

        articles = []
        for data in articles_data:
            try:
                # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
                publish_at = None
                if data.get('publishAt'):
                    publish_at = date_parser.parse(data['publishAt'])
                else:
                    publish_at = datetime.now()  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

                note_id = data['id']
                # URLã‚’æ­£ã—ãç”Ÿæˆ
                article_url = f"https://note.com/{self.username}/n/{note_id}"

                article = Article(
                    id=note_id,
                    key=note_id,  # keyã¯è¨˜äº‹ID
                    title=data.get('name', 'Untitled'),
                    publish_at=publish_at,
                    eyecatch_url=data.get('eyecatch'),
                    url=article_url
                )
                articles.append(article)
            except Exception as e:
                logger.warning(f"è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}")
                continue

        # å…¬é–‹æ—¥é †ã«ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„é †ï¼‰
        articles.sort(key=lambda x: x.publish_at)

        # ãƒ‡ãƒãƒƒã‚°: è¨˜äº‹ã®é †åºã‚’ç¢ºèª
        if logger.level <= logging.DEBUG:
            logger.debug("è¨˜äº‹ãƒªã‚¹ãƒˆã®é †åºï¼ˆå…¬é–‹æ—¥é †ï¼‰:")
            for i, a in enumerate(articles, 1):
                title_preview = (a.title[:30] if a.title else 'Untitled')
                logger.debug(f"  {i}. {a.id} - {a.publish_at} - {title_preview}")

        logger.info(f"âœ“ {len(articles)}ä»¶ã®è¨˜äº‹ã‚’æ¤œå‡º")
        return articles

    def scrape_article_detail(self, article: Article) -> ArticleDetail:
        """è¨˜äº‹è©³ç´°ã‚’å–å¾—"""
        logger.info(f"\nè¨˜äº‹å–å¾—ä¸­: {article.title if article.title != 'Untitled' else article.id}")
        logger.info(f"  URL: {article.url}")

        response = self.fetch_with_retry(article.url)
        html = response.text
        soup = BeautifulSoup(html, 'lxml')

        # JSON-LDãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        json_ld = self.parser.extract_json_ld(html)

        # ã‚¿ã‚¤ãƒˆãƒ«ã¨å…¬é–‹æ—¥ã‚’JSON-LDã‹ã‚‰å–å¾—ï¼ˆNoneã®å ´åˆï¼‰
        title = article.title
        publish_at = article.publish_at
        eyecatch_url = article.eyecatch_url

        if json_ld:
            if not title or title == 'Untitled':
                title = json_ld.get('headline', json_ld.get('name', 'Untitled'))
            if not publish_at or publish_at == datetime.now():
                date_str = json_ld.get('datePublished')
                if date_str:
                    try:
                        publish_at = date_parser.parse(date_str)
                    except:
                        pass
            if not eyecatch_url:
                image = json_ld.get('image')
                if image:
                    if isinstance(image, dict):
                        eyecatch_url = image.get('url')
                    elif isinstance(image, list) and len(image) > 0:
                        eyecatch_url = image[0].get('url') if isinstance(image[0], dict) else image[0]
                    elif isinstance(image, str):
                        eyecatch_url = image

        # metaã‚¿ã‚°ã‹ã‚‰ã‚‚å–å¾—ã‚’è©¦ã¿ã‚‹
        if not title or title == 'Untitled':
            og_title = soup.find('meta', property='og:title')
            if og_title:
                title = og_title.get('content', 'Untitled')

        # æ›´æ–°æ—¥æ™‚ã‚’å–å¾—ï¼ˆJSON-LDã‹ã‚‰ï¼‰
        date_modified = None
        if json_ld:
            date_modified = json_ld.get('dateModified')

        logger.info(f"  ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        logger.info(f"  å…¬é–‹æ—¥: {publish_at.strftime('%Y-%m-%d') if publish_at else 'Unknown'}")
        if date_modified:
            logger.info(f"  æ›´æ–°æ—¥: {date_modified}")

        # è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—
        body_html = self.parser.extract_article_body(soup)

        # HTMLâ†’Markdownå¤‰æ›
        body_markdown = self.converter.convert(body_html)

        # ç”»åƒURLã‚’æŠ½å‡º
        image_urls = self.converter.extract_image_urls(body_html)
        if eyecatch_url:
            image_urls.insert(0, eyecatch_url)

        logger.info(f"  ç”»åƒ: {len(image_urls)}æšæ¤œå‡º")

        return ArticleDetail(
            id=article.id,
            key=article.key,
            title=title,
            publish_at=publish_at,
            eyecatch_url=eyecatch_url,
            url=article.url,
            body_html=body_html,
            body_markdown=body_markdown,
            image_urls=image_urls,
            json_ld=json_ld,
            date_modified=date_modified
        )

    def load_local_articles(self) -> Dict[str, dict]:
        """ãƒ­ãƒ¼ã‚«ãƒ«ã®æ—¢å­˜è¨˜äº‹æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€ï¼ˆarticle_id -> frontmatterï¼‰"""
        local_articles = {}

        if not self.output_dir.exists():
            return local_articles

        for filepath in self.output_dir.glob('*.md'):
            frontmatter = MarkdownGenerator.parse_frontmatter(filepath)
            if frontmatter and 'article_id' in frontmatter:
                local_articles[frontmatter['article_id']] = {
                    'frontmatter': frontmatter,
                    'filepath': filepath
                }

        return local_articles

    def run(self, max_articles: Optional[int] = None, start_day: int = 1,
            skip_existing: bool = False, update_check: bool = False):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        fetched_at = datetime.now()

        logger.info("=" * 60)
        logger.info("note.comè¨˜äº‹å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
        logger.info("=" * 60)

        # ãƒ­ãƒ¼ã‚«ãƒ«è¨˜äº‹ã‚’èª­ã¿è¾¼ã‚€ï¼ˆday_numberæ±ºå®šã®ãŸã‚å¸¸ã«å¿…è¦ï¼‰
        local_articles = self.load_local_articles()
        if update_check:
            logger.info("æ›´æ–°ãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªä¸­...")
            logger.info(f"âœ“ {len(local_articles)}ä»¶ã®ãƒ­ãƒ¼ã‚«ãƒ«è¨˜äº‹ã‚’æ¤œå‡º\n")
        elif local_articles:
            logger.debug(f"æ—¢å­˜ãƒ­ãƒ¼ã‚«ãƒ«è¨˜äº‹: {len(local_articles)}ä»¶æ¤œå‡º")

        # è¨˜äº‹ä¸€è¦§ã‚’å–å¾—
        articles = self.fetch_article_list()

        if max_articles:
            articles = articles[:max_articles]

        logger.info(f"\n{len(articles)}ä»¶ã®è¨˜äº‹ã‚’å‡¦ç†ã—ã¾ã™\n")

        # çµ±è¨ˆ
        stats = {'new': 0, 'updated': 0, 'skipped': 0}

        # æ—¢å­˜è¨˜äº‹ã®æœ€å¤§day_numberã‚’å–å¾—
        max_existing_day = 0
        if local_articles:
            for info in local_articles.values():
                day_num = info['frontmatter'].get('day_number', 0)
                if day_num > max_existing_day:
                    max_existing_day = day_num
            logger.debug(f"æ—¢å­˜è¨˜äº‹ã®æœ€å¤§dayç•ªå·: {max_existing_day}")

        # æ–°è¦è¨˜äº‹ã‚’ç‰¹å®šã—ã€date_modifiedé †ã§ã‚½ãƒ¼ãƒˆã—ã¦day_numberã‚’äº‹å‰ã«å‰²ã‚Šå½“ã¦ã‚‹
        # ã¾ãšå„æ–°è¦è¨˜äº‹ã®date_modifiedã‚’å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        new_articles_with_date = []
        for article in articles:
            if article.id not in local_articles:
                # è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰date_modifiedã‚’å–å¾—
                logger.info(f"æ–°è¦è¨˜äº‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­: {article.id}")
                try:
                    response = self.fetch_with_retry(article.url)
                    html = response.text
                    json_ld = self.parser.extract_json_ld(html)
                    date_modified = json_ld.get('dateModified') if json_ld else None
                    new_articles_with_date.append({
                        'article': article,
                        'date_modified': date_modified
                    })
                    logger.debug(f"  {article.id}: date_modified = {date_modified}")
                except Exception as e:
                    logger.warning(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•— ({article.id}): {e}")
                    new_articles_with_date.append({
                        'article': article,
                        'date_modified': None
                    })

        # æ–°è¦è¨˜äº‹ã‚’date_modifiedæ˜‡é †ã§ã‚½ãƒ¼ãƒˆï¼ˆNoneã¯æœ€å¾Œã«ï¼‰
        new_articles_with_date.sort(key=lambda x: x['date_modified'] or '9999-99-99')

        # æ–°è¦è¨˜äº‹ã«day_numberã‚’äº‹å‰å‰²ã‚Šå½“ã¦ï¼ˆarticle_id -> day_numberï¼‰
        new_article_day_map = {}
        next_day = max_existing_day + 1 if max_existing_day > 0 else start_day
        for item in new_articles_with_date:
            new_article = item['article']
            new_article_day_map[new_article.id] = next_day
            logger.debug(f"  ğŸ†• æ–°è¦è¨˜äº‹ {new_article.id} (date_modified: {item['date_modified']}) â†’ day{next_day:04d} ã«äº‹å‰å‰²ã‚Šå½“ã¦")
            next_day += 1

        # å„è¨˜äº‹ã‚’å‡¦ç†
        for idx, article in enumerate(articles, start=start_day):
            try:
                # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
                if skip_existing:
                    filename = MarkdownGenerator.generate_filename(idx, article.title, article.id)
                    if (self.output_dir / filename).exists():
                        logger.info(f"ã‚¹ã‚­ãƒƒãƒ— (æ—¢å­˜): {article.title}")
                        stats['skipped'] += 1
                        continue

                # æ›´æ–°ãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰
                should_fetch = True
                if update_check and article.id in local_articles:
                    # ã¾ãšè¨˜äº‹ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦dateModifiedã‚’ç¢ºèª
                    logger.info(f"\næ›´æ–°ãƒã‚§ãƒƒã‚¯ä¸­: {article.id}")
                    logger.info(f"  âš¡ è»½é‡ãƒã‚§ãƒƒã‚¯: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—ï¼ˆæœ¬æ–‡ãƒ»ç”»åƒã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                    response = self.fetch_with_retry(article.url)
                    html = response.text
                    json_ld = self.parser.extract_json_ld(html)

                    web_date_modified = json_ld.get('dateModified') if json_ld else None
                    local_date_modified = local_articles[article.id]['frontmatter'].get('date_modified')

                    if web_date_modified and local_date_modified:
                        if web_date_modified == local_date_modified:
                            logger.info(f"  âœ“ æ›´æ–°ãªã—: {web_date_modified}")
                            logger.info(f"  ğŸ’¾ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœ¬æ–‡ãƒ»ç”»åƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å›é¿ï¼‰")
                            stats['skipped'] += 1
                            should_fetch = False
                        else:
                            logger.info(f"  ğŸ”„ æ›´æ–°æ¤œå‡º: {local_date_modified} â†’ {web_date_modified}")
                            logger.info(f"  ğŸ“¥ å†å–å¾—ã‚’é–‹å§‹...")
                            stats['updated'] += 1
                    else:
                        # dateModifiedãŒãªã„å ´åˆã¯å–å¾—
                        logger.info(f"  âš  æ›´æ–°æ—¥æ™‚æƒ…å ±ãªã— - å†å–å¾—ã—ã¾ã™")
                        stats['updated'] += 1

                if not should_fetch:
                    continue

                # dayç•ªå·ã®æ±ºå®š
                if article.id in local_articles:
                    # æ—¢å­˜è¨˜äº‹: ãƒ­ãƒ¼ã‚«ãƒ«ã®dayç•ªå·ã‚’ä¿æŒ
                    day_number = local_articles[article.id]['frontmatter'].get('day_number', idx)
                    logger.debug(f"  ğŸ”„ æ—¢å­˜è¨˜äº‹ã® day{day_number:04d} ã‚’ä¿æŒ")
                elif article.id in new_article_day_map:
                    # æ–°è¦è¨˜äº‹: äº‹å‰å‰²ã‚Šå½“ã¦ãƒãƒƒãƒ—ã‹ã‚‰å–å¾—
                    day_number = new_article_day_map[article.id]
                    stats['new'] += 1
                    logger.debug(f"  ğŸ†• æ–°è¦è¨˜äº‹ã¨ã—ã¦ day{day_number:04d} ã«å‰²ã‚Šå½“ã¦")
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: enumerateã®idxã‚’ä½¿ç”¨
                    day_number = idx
                    stats['new'] += 1

                # è¨˜äº‹è©³ç´°ã‚’å–å¾—
                detail = self.scrape_article_detail(article)

                # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                if detail.image_urls:
                    logger.info(f"  ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
                    url_map = self.image_downloader.download_images(detail.id, detail.image_urls)

                    # Markdownã®URLã‚’ç½®æ›
                    detail.body_markdown = self.image_downloader.replace_image_urls(
                        detail.body_markdown, url_map
                    )

                # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                MarkdownGenerator.save_article(
                    detail, day_number, detail.body_markdown, self.output_dir, fetched_at,
                    date_modified=detail.date_modified
                )

                time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

            except Exception as e:
                logger.error(f"âœ— ã‚¨ãƒ©ãƒ¼ ({article.title}): {e}")
                continue

        # å‡¦ç†æ™‚é–“ã‚’è¨ˆç®—
        elapsed_time = datetime.now() - fetched_at

        logger.info("\n" + "=" * 60)
        logger.info("å®Œäº†ï¼")
        logger.info("=" * 60)
        logger.info(f"å‡ºåŠ›å…ˆ: {self.output_dir}")
        logger.info(f"ç”»åƒ: {self.image_dir}")
        logger.info(f"å‡¦ç†æ™‚é–“: {elapsed_time.total_seconds():.1f}ç§’")

        if update_check:
            logger.info(f"\nğŸ“Š æ›´æ–°ãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰çµ±è¨ˆ:")
            logger.info(f"  ğŸ†• æ–°è¦è¨˜äº‹: {stats['new']}ä»¶")
            logger.info(f"  ğŸ”„ æ›´æ–°ã•ã‚ŒãŸè¨˜äº‹: {stats['updated']}ä»¶")
            logger.info(f"  â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {stats['skipped']}ä»¶")

            if stats['skipped'] > 0:
                logger.info(f"\nğŸ’¡ åŠ¹ç‡åŒ–:")
                logger.info(f"  {stats['skipped']}ä»¶ã®è¨˜äº‹ã§æœ¬æ–‡ãƒ»ç”»åƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å›é¿")
                logger.info(f"  æ¨å®šãƒ‡ãƒ¼ã‚¿å‰Šæ¸›: ~{stats['skipped'] * 100}KBï¼ˆç”»åƒåˆ†ï¼‰")
        else:
            total_articles = stats.get('new', 0) + stats.get('updated', 0) + stats.get('skipped', 0)
            if total_articles > 0:
                logger.info(f"\nğŸ“Š å‡¦ç†çµ±è¨ˆ: {total_articles}ä»¶ã®è¨˜äº‹ã‚’å–å¾—")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='note.comè¨˜äº‹å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        '--username',
        default='yusukemori_ravi',
        help='note.comãƒ¦ãƒ¼ã‚¶ãƒ¼å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: yusukemori_ravi)'
    )
    parser.add_argument(
        '--output-dir',
        type=Path,
        default=Path('./articles'),
        help='Markdownå‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ./articles)'
    )
    parser.add_argument(
        '--image-dir',
        type=Path,
        default=Path('./images'),
        help='ç”»åƒä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ./images)'
    )
    parser.add_argument(
        '--max-articles',
        type=int,
        default=None,
        help='å–å¾—ã™ã‚‹æœ€å¤§è¨˜äº‹æ•°'
    )
    parser.add_argument(
        '--start-day',
        type=int,
        default=1,
        help='é–‹å§‹æ—¥ç•ªå· (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1)'
    )
    parser.add_argument(
        '--skip-existing',
        action='store_true',
        help='æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—'
    )
    parser.add_argument(
        '--update-check',
        action='store_true',
        help='æ›´æ–°ãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¨Webå´ã®dateModifiedã‚’æ¯”è¼ƒã—ã€æ›´æ–°ã•ã‚ŒãŸè¨˜äº‹ã®ã¿å–å¾—'
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º'
    )

    args = parser.parse_args()

    if args.verbose:
        logger.setLevel(logging.DEBUG)

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’å®Ÿè¡Œ
    scraper = NoteArticleScraper(
        username=args.username,
        base_dir=Path.cwd(),
        image_dir=args.image_dir,
        output_dir=args.output_dir
    )

    scraper.run(
        max_articles=args.max_articles,
        start_day=args.start_day,
        skip_existing=args.skip_existing,
        update_check=args.update_check
    )


if __name__ == '__main__':
    main()
